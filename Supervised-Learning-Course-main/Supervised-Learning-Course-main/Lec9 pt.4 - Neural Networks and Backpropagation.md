# Neural Networks and Backpropagation

Recall from the previous part, this is what backpropagation looked like for one node/classifier:

![image](https://github.com/SohailaDiab/Supervised-Learning-Course/assets/70928356/9739352d-1bbc-40d2-9495-f642639d30f4)

This is how this one node would look like, if more compact:

![image](https://github.com/SohailaDiab/Supervised-Learning-Course/assets/70928356/548e7b04-818c-4aa9-9806-3a0f6b2b2517)

> This is one node, meaning it is **ONE CLASSIFIER**.

## Fully Connected/Dense Neural Network

![image](https://github.com/SohailaDiab/Supervised-Learning-Course/assets/70928356/27059687-c621-4edc-ab90-708363478c87)

Here, we have **multiple classifiers**; 3 in hidden layer 1.
> Note: Each node is considered a classifier.

### How does this help?
First of all, what happens if ALL the weights in all the nodes got initialized to 0?
- They will all start at the same point, and move down the slope together in the exact same way.
- Therefore, having multiple nodes/classifiers is useless when we initialize all weights to 0.

The advantage we get from having multiple nodes is that we initialize all the weights randomly, which lets each classifier start from a different point in the weight-cost slope. This leads to each classifier to learn differently.
> Yes, it will be more computationally exhaustive, but it allows the model to learn more complex functions.

## Back Propagation in Dense Neural Network
< TO BE COMPLETED D: >
  
![image](https://github.com/SohailaDiab/Supervised-Learning-Course/assets/70928356/486b7568-cd67-4060-9413-bbfe978a17fc)


























<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
